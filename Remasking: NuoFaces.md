# Remasking: The Nuo Faces

<img width="1534" height="861" alt="POSTER" src="https://github.com/user-attachments/assets/c33d83a3-0678-4546-93cf-1e0beef0a202" />

---
## Overview & Project Description

### Project Description
Nuo culture originated from the natural worship, totemic beliefs, and shamanistic practices of the early Han people, and constitutes an important part of traditional Chinese culture with strong religious and artistic significance. This project draws inspiration from the historical legacy of Nuo culture, selecting representative Nuo opera masks as core visual elements for artistic reinterpretation and re-creation. A dynamic deformation system based on real-time interaction was designed to endow the traditional masks with renewed vitality.

To deepen the cultural narrative, four Nuo masks were chosen, each embodying a distinct symbolic meaning: calmness (wise figures), humour (folk jesters), solemnity (guardian deities), and ferocity (demon expellers). Through hand gesture control, users can not only switch between different mask models but also manipulate the dynamic processes of fragmentation and aggregation, enhancing the overall immersive experience. This dynamic transformation metaphorically reflects the fragmentation and reconstruction of traditional culture over time, while demonstrating the ongoing vitality and relevance of Nuo culture in a contemporary context.

<img width="1604" height="553" alt="d98ad2c6ecadfbce39a083fb3dfdc53e" src="https://github.com/user-attachments/assets/4b99871e-ae7e-4200-8a07-2e1e7c3a7dc4" />

---

### Interaction Design
The project develops multiple interaction logics based on distinct hand gestures (shown in the figure below). When the user performs a pinching gesture with one hand (e.g., the right hand) by bringing the thumb and index finger together, the system detects the change in fingertip distance and triggers the switching of mask models. Meanwhile, the other hand (e.g., the left hand) controls the aggregation and dispersion of the current mask by opening or pinching the thumb and index finger, producing dynamic visual effects of fragmentation and reformation. Through this dual-hand coordination, users can intuitively manipulate the morphological changes of the masks, enhancing the immersive quality and participatory nature of the experience. This approach offers new possibilities for the contemporary reinterpretation of Nuo culture within a digital media environment.

<img width="1024" height="1024" alt="adec82b10be3a39babe1bdded8377c4a" src="https://github.com/user-attachments/assets/b101f7ee-2b99-46ec-bc71-df5ed81d7d87" />
*This image was generated by AI*

---

## Supplies & Materials

This section outlines the software tools, resource sources, and visual documentation used in the technical implementation of the project.

### Software Tools Used：
TouchDesigner 2023.12120 (primary development platform), MeshLab (for point cloud processing of models), MediaPipe (for real-time face and hand tracking), and Blender/Rhino (for model pre-processing and format conversion).

TouchDesigner served as the primary development environment, supplemented by various software tools for model processing and optimisation to meet the demands of real-time interaction and visual performance.

The project adopts the MediaPipe framework as the core interaction technology, enabling real-time recognition and tracking of facial features and finger points in front of the computer screen. Based on the captured data, the system first achieves precise three-dimensional positioning of the mask, allowing it to dynamically align with the user's facial position and orientation, thus ensuring a natural and immersive interaction process. In addition, by detecting specific gestures involving the thumb and index finger of both hands, the system enables the switching of mask models and the dynamic deformation processes of aggregation and fragmentation, providing users with intuitive and responsive control over mask transformations.

Overall, the project integrates MediaPipe's human keypoint detection capabilities with TouchDesigner’s real-time visualisation environment, effectively enhancing the representation of traditional cultural elements within a digital interactive context and exploring new pathways for the contemporary reinterpretation and regeneration of conventional art forms.

Source of downloaded assets:
All mask models were sourced from resources shared by previous undergraduate cohorts and were subsequently refined, processed, and optimised for use in this project.

### Images:  
<img width="1534" height="861" alt="bb75893b32251b1a509e1d0e59acf7d0" src="https://github.com/user-attachments/assets/e461758b-a793-4636-bf07-3a9e001a7195" />
*Mask dispersing into fragments*

<img width="1502" height="841" alt="22c8410a92e3f89c0ed736b7773722c6" src="https://github.com/user-attachments/assets/661cb20e-6d23-4a47-b816-320840f3a83a" />
*Mask reassembling into its original form*

<img width="1088" height="849" alt="655b80b8986133e405b6d1728aa8b0c7" src="https://github.com/user-attachments/assets/c754d1f3-4547-4731-bdbc-9be1b01c773c" />
*Transition between different masks*

---

## Process

The following outlines the complete process of our project development:

**1.MediaPipe Framework Construction**

At the early stage of the project, a real-time tracking system was built using MediaPipe to detect facial features and hand key points, forming the basis for interactive control. The system employs the 'Hand Tracking' and 'Face Mesh' modules to capture gestures and facial orientation.

Gesture recognition focuses on the thumb and index finger; changes in their distance are used to trigger functions such as model switching and mask deformation. Facial tracking enables the virtual mask to align and follow the user’s face in 3D space.

The data is transmitted to TouchDesigner via a custom channel. Within TD, CHOP nodes such as 'Math', 'Select', and 'Logic' process the input for real-time mapping to mask position, switching, and transformation control.

<img width="2479" height="1242" alt="0a15569fb9f3efad540cef8c06bbb813" src="https://github.com/user-attachments/assets/28cddcf6-d28c-441a-b734-07d0285aae6e" />

**2.Model File Format Conversion**

As MeshLab only supports the import of glTF files, and the original model was in .skp format, the model was first converted to glTF using Blender. This completed the initial stage of model preprocessing and prepared the file for subsequent point cloud generation.

**3.Point Cloud Generation and Import**

The model was exported in .ply format after point cloud conversion, retaining both XYZ coordinates and RGB colour data to support accurate visual reconstruction.

In TouchDesigner, the file was imported via the 'Point File In' component, with Position and Colour channels enabled. A 'Point Transform' node was used to normalise coordinates, ensuring consistent scale and alignment across models for later transformation and interaction.

To optimise performance and maintain visual consistency, all mask models were sampled with similar point counts (200000) and selectively simplified to reduce rendering load without compromising key features.

<img width="2386" height="1242" alt="1e8e003a2b016f9d7995d346a3e77710" src="https://github.com/user-attachments/assets/4a6866c9-55df-4d45-86b8-057affe48270" />

**4.Construction of the Basic Structure**

<img width="2556" height="1275" alt="75dd792837f130e9a3bcb0bade3e4bdc" src="https://github.com/user-attachments/assets/e9d84da1-7c4c-4701-bd66-4db86334b8a7" />

<img width="2266" height="1194" alt="258fdd74d43a9418b8704205e429ce9b" src="https://github.com/user-attachments/assets/7eb6fd81-71d5-4a43-9fd0-5b555616d25a" />

The TouchDesigner framework consists of four main modules: input recognition, data processing, mask control, and visual output. The system integrates data flow and node-based logic to enable dynamic model switching and fragmentation effects, driven by real-time gesture input.

The input module uses the MediaPipe framework to track facial landmarks and hand key points. Pinching and spreading gestures between the thumb and index finger are detected to generate control signals.

In the data processing module, one hand controls mask switching, while the other controls fragmentation and aggregation. These gestures are processed through a chain of CHOP nodes, including 'Math', 'Select', and 'Logic', to generate corresponding control outputs.

In the mask control module, four Nuo masks are imported as point clouds via the 'Point File Select' node. Model switching is handled by the 'Switch' node, while transformation chains and logic drive the movement of fragments. A 'Noise' node introduces randomised motion, and a 'Limit' node constrains the displacement range. 'Add' and 'Line' nodes generate connecting lines between fragments to form structural links.

The visual output module renders the final geometry using the 'Render' node, with additional post-processing effects applied via 'Composite' and 'Displace' nodes before display.

**5.Project Testing and Iteration**

Following the completion of the basic system architecture, multiple rounds of visual and interaction testing were carried out to ensure optimal presentation of the project. In visual testing, parameters across key components were repeatedly adjusted, with a particular focus on the clarity and coherence of mask transitions, fragmentation, and reassembly. The aim was to maintain recognisable mask shapes during transformation, ensure smooth fragment movement, and keep connection lines legible without visually interfering with the main content.

Interaction testing focused primarily on the alignment between the virtual mask and the user’s face. To ensure natural fit across different users, the mask’s initial position, scale, and placement in 3D space were refined based on the detected facial centre and contour data.


**6.Final Adjustments and Optimisation**

During the Week 10 class, feedback from tutors and insights gained from viewing other students’ projects led to a period of reflection and refinement.

Although the visual design was already detailed, it was noted that the background lacked visual impact and engagement. The existing background—live camera feed—was perceived as too plain. In response, a dynamic background effect was introduced by applying a 'Noise' TOP to a 'Displace' TOP. The noise pattern was driven by the expression 'absTime.seconds', enabling continuous temporal variation. This created a flowing distortion of background pixels, further enhanced by adjusting the 'Displace Weight' and 'Offset Weight' parameters. The effect resembled textured frosted glass, softening background detail while emphasising the foreground mask fragments and connection lines.

During the interaction, the displacement effect responds to the mask state: when the mask disperses into fragments, the 'Displace' module is activated, producing a blurred and drifting visual atmosphere that aligns with the themes of fragmentation and fluidity. When the mask reforms, the displacement is reduced or disabled, and the background returns to clarity, reinforcing the cohesion and symbolic weight of the reassembled form.

<img width="1516" height="1308" alt="bbeba4507a9eec06d9d57b7d8a358eb0" src="https://github.com/user-attachments/assets/7d46e8d4-f511-4367-bc45-a1bc95d46ce2" />

---

## Video Demonstration

Link to a demo video: https://mega.nz/file/ezIRGQaA#g-MqgNlmmLJ_0594nts-dYLRvE2y8hD6SSVgnLvb9mA

---

## Final Images

<img width="2262" height="1277" alt="6fc2b7351665d3926a265cd91ba7a79b" src="https://github.com/user-attachments/assets/8c29eee3-48bf-4dce-94ef-8422ddb55a4c" />

<img width="1920" height="1080" alt="3127a42c447a69353b5b365632ba4c0b" src="https://github.com/user-attachments/assets/7d34dae3-bcd5-4232-ad19-89ed01048b28" />

<img width="1920" height="1080" alt="2df7d19bfbe77d2411e77e05c2ff233b" src="https://github.com/user-attachments/assets/2db5660f-74c1-42bd-95a0-139837a27881" />

---

## Link to TouchDesigner Files

MEGA network link: https://mega.nz/file/uzAWgYBQ#TO9NniCItDmKodwy5gw9s_8jpeVthAHpckivScUcMe0

---
